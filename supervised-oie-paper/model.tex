\begin{figure*}[t!]

  \centering
    \includegraphics[width=1\textwidth]{figures/rnn}

    \caption{RNN model architecture. The figure depicts a single layer of LSTM, while it is possible to
      chain several in succession. The blue dotted line represents predicate features which are duplicated and concatenated to all other word features. The circles indicate the different features in the vector per current word and predicate: word-embedding, part of speech embedding, and position index.}
  \label{fig:architecture}

\end{figure*}
%\todo{Include encoding / decoding (with confidence computation) to Figure \ref{fig:architecture}}

Our supervised Open IE model is inspired by the state of the art deep learning approach for SRL suggested by \newcite{baidusrl} (elaborated in Section \ref{sec:related}).
Follwoing this approach is compelling thanks to its (relative) simplicity and the close resemblance between SRL and Open IE, as discussed above. 
%\todo{commented text repeats end of sec 2}
% enabling us to extend and mold it into a supervised
% Open IE extractor.

We next detail the modeling and design choices we make in
(1) encoding of the input data,
(2) the feature extraction and model architecture,
(3) decoding Open IE extractions during inference, and 
(4) techinical details of our implementation.
\input{figures/dist}

\subsection{Data Encoding}
\label{sec:encoding}
%% \begin{itemize}
%%   \item We choose to model Open IE as a word level annotation task.

%%   \item We devise a variant on the well known BIO (Beginning, Inside, Outside)  annotation \cite{ramshaw1995text,sang1999representing},
%%   often used in sequence labeling tasks, such as noun phrase chunking, named entity recognition,
%%   or SRL.
%%   This fits our task, as all predicates consist of consecutive, non overlapping span of words.

%%   \item Specifically, each predicate (all verbs, in our case) in the sentence produces an encoding of all of the words
%%   in the sentence relative to their role for the specific target predicate.

%%   \item With regards to a specific target predicate, each word can either be at the start or inside a predicate / argument span, or outside of both.
%%   See Table \ref{tab:oie_examples} for various encoding examples.

%%   \item Each argument is encoded using its index \emph{in the Open IE tuple}.
%%   Note that this encodes semantic information, as these are often rearranged to form
%%   a correct and coherent proposition.
%%   See the last example in Table \ref{tab:oie_examples} in which the order of arguments
%%   in the Open IE tuple deviates from the ordering in the original sentence due to
%%   a relative clause construction (headed by the word ``Brexit'').

%%   \item A special case occurs when a single predicate yields multiple propositions.
%%   This happens in several linguistic and syntactic constructions, such as apposition, co-ordination, or coreference.

%%   \item We handle this by when encoding the predicate by repeating the argument label per the
%%   each tuple it appears in.

%%   \item For example, see the second example in the table shows this phenomenon happening in appositive
%%   construction.

%% \end{itemize}

We choose to model Open IE as a word level annotation task. We follow,  and slightly modify, the well known BIO (Beginning, Inside, Outside)  annotation \cite{ramshaw1995text,sang1999representing},
often used in sequence labeling tasks such as noun phrase chunking, named entity recognition and SRL.
%\todo{notice I changed the terminology to refer consistently to  "predicate", rather that "verb", to make it general}
In our scenario, we annotate a sentence in multiple passes, each time with respect to another targeted predicate within it. Each such target predicate induces an encoding of all other words
in the sentence, which specifies the role of each word with respect to the target predicate.
Concretely, each word can be either at the beginning or inside of either the predicate or an argument span, or outside of both.
%\todo{I'm not sure for the meaning of the following sentence. Why is it important that the predicates are consecutive and non-overlapping? Worth clarifying}
This fits our data, as all our predicates and arguments consist of consecutive, non overlapping spans of words, which BIO labeling is capable of represnting.
See Table \ref{tab:oie_examples} for various encoding examples.

As seen in the table, the encoding identifies each argument by specifying its index (position) in the Open IE tuple.
Note that this ordering captures certain semantic information, as the arguments in a tuple are often rearranged relative to their original order in the sentence in order to form
a correct and coherent proposition when reading the standalone tuple. Thus, an agent would typically be the first argument, a theme the second one and so on, even though the Open IE ordering is looser and less consistent than the pre-defined role labels in SRL.
For instance, see the last example in Table \ref{tab:oie_examples}, in which the order of the arguments
in the Open IE tuple deviates from the ordering in the original sentence due to
a relative clause construction (headed by the word ``Brexit'').

%\todo{Please check that I phrased it properly}
A special case for argument encoding occurs when a single target predicate yields multiple propositions.
This might happen in several linguistic and syntactic constructions, such as apposition, co-ordination and coreference, as illustrated in the second example in the table. As can be seen in the encoding of this example, we assign the same argument index to all arguments in that position, coming from the different propositions headed by the current target predicate. This process is reversed later during decoding, at inference time (Subsection \ref{subsec-inference}).

\subsubsection{Dealing with label imbalance}
%\todo{I wonder if this should be 3.1.1, also fitting the section outline preceding 3.1}
%% \begin{itemize}
%% \item Applying this label scheme on the train partition of the  Open IE data set required the use
%% of 15 labels.
%% Two labels for predicates - \oielabel{P-B}, and \oielabel{P-I}.
%% 12 labels for arguments, as the maximum observed number of arguments was 6, and each of which requires 2 labels.
%% Finally, one label (\oielabel{O}) is used for representing the Outside label.

%% \item Examining the label distribution in the Open IE corpus (Figure \ref{fig:orig_label_dist})
%% reveals a heavy bias towards the \oielabel{O} label.

%% \item This is expected, as most words will not be participate in a predicate or any of the arguments for a given frame.

%% \item Initial experiments showed that this heavy bias towards the Outside label poses a problem.
%% The training process kept converging on a model which predicted the majority class for all words, regardless of their context.

%% \item To cope with this problem, we introduced an artificial division of the of the \oielabel{O}
%% label into @ classes, based on the last observed non-Outside label (i.e., predicate or argument label).
%% In addition, we introduce two more ``Outside'' labels for when the Outside label appeared at the beginning of the sentence (before any non-Outside label was observed)
%% or a the end of it (when these the remainder of the labels are outside of any predicate or argument).
%% For example, revisiting the first example in Table \ref{tab:oie_examples}, our revised annotation would assign the following labels: @

%% \item Recalculating the label distribution with the artificially extended set shows
%% a much more balanced partition (Figure \ref{fig:new_label_dist}.
%% As we will show in Section \ref{sec:evaluation}, this extension indeed enables our model to
%% learn from the variability in the data and converge on more useful weight configurations.

%% \end{itemize}

Applying the above labeling scheme on the train partition of the Open IE data set required the use
of 15 labels:
Two labels for predicates - \oielabel{P-B} and \oielabel{P-I}, and 12 labels for arguments, as the maximum observed number of arguments was 6 and each of which requires 2 labels (Beginning and Inside). Finally, one label (\oielabel{O}) is required for representing the Outside label.

Examining the resulting label distribution in the Open IE corpus (Figure \ref{fig:orig_label_dist})
reveals a heavy bias towards the \oielabel{O} label, corresponding to over half of the words.
This is an expected property of our per-predicate encoding, since each sentence is typically composed of several propositions, headed by different predicates. Accordingly, most sentence words will not be related to any single target predicate. 

Initial experiments showed that this heavy bias towards the Outside label poses a problem for our classification algorithm.
The training process kept converging on a model that predicted the majority class for all words, regardless of their context.
To cope with this problem, we introduced an artificial division of the \oielabel{O}
label into 16 classes. 14 of those correspond to the last observed non-Outside label (i.e., predicate or argument label).
In addition, we introduce two more ``Outside'' labels: \oielabel{O-S} for when the Outside label appeared at the beginning of the sentence (before any non-Outside label was observed)
and \oielabel{O-E} at the end of the sentence (when the remainder of the words are outside of any predicate or argument).
For example, our revisited annotation for the second example in Table \ref{tab:oie_examples} (first proposition) would be:

%% \begin{figure}
%% \ann{The}{A0-B} \ann{president}{A0-I} \ann{claimed}{P-B} \ann{that}{P-I} \ann{he}{P-I} \ann{won}{P-I}
%% \ann{the}{A1-B} \ann{majority}{A1-I} \ann{vote}{A1-I}
%% \end{figure}

\begin{figure}[H]
%  \vspace{-1\baselineskip}
  \framebox{\parbox{\dimexpr\linewidth-2\fboxsep-2\fboxrule}{
                 \ann{Barack}{A0-B} \ann{Obama}{A0-I} \ann{,}{O-A0} \ann{a}{O-A0} \ann{former}{O-A0} \ann{U.S.}{O-A0} \ann{president}{O-A0} \ann{,}{O-A0} \ann{was}{P-B} \ann{born}{P-I} \ann{in}{P-I} \ann{Hawaii}{A1-B}

    
}}
%\vspace{-1\baselineskip}
\end{figure}


Following this label manipulation, we recalculate the label distribution and find
a much more balanced partition (Figure \ref{fig:new_label_dist}).
As we will show in Section \ref{sec:evaluation}, this manipulation indeed enables our learning process to
converge on a more useful model.
Previous models have dealt with label bias in various other techniques, which might be better suited
for tasks with smaller label sets. These methods include for example:
relabling with BILOU labels \cite{ratinov2009design},
label resampling \cite{estabrooks2004multiple},
or CRF for conditioning current prediction on previous model decisions \cite{baidusrl}.

\subsection{Model Architecture and Feature Extraction}
Given this task formulation and the fact that the sentences are of arbitrary length,
it is appealing to model the task using Recurrent Neural Networks (RNNs).
This approach was proven to be effective in many recent NLP models 
(see \cite{goldberg2015primer} for a recent extensive survey of RNNs in NLP).

%\todo{is it really a deep transducer? In the figure there's only one layer of LSTM in each direction}
Specifically, we use a bi-directional deep LSTM transducer \cite{graves2012sequence},
which takes into account arbitrary length contexts from both past and future words.
The input to the LSTM is a raw sentence and a target predicate (a verb in our implementation),
and its output is a probability distribution, per word, over the possible labels with respect to the target predicate.

The complete architecture of our LSTM model is presented in Figure \ref{fig:architecture}. The input features per word in the sentence are:
\begin{enumerate}
\item
300 dimensional word embedding vectors for both the current word and the target predicate, taken from the same embedding space.
That is, a predicate word would have the same
representation whether it is considered as the target predicate or not.
%\todo{the commented text doesn't seem needed, it's quite clear}
% This happens in case where there is more than one predicate in the sentence as in example @ in
% Table \ref{tab:oie_examples}.

\item 5 dimensional embedding of the part of speech of the current word and the target predicate, and
\item The position index of the word and the target predicate in the sentence.

\end{enumerate}
%% \todo{In the figure each word, and the predicate, split to two circles and it's not clear what each of them stands for. I thought they represent the embedding components, but there are three components.}
%\todo{Is it OK not to give the network equations?}

\subsection{Inference}
\label{subsec-inference}

%% \begin{itemize}
%% \item During inference, we decode Open IE extractions from the most probable label assignment for each word,
%% as predicted by our LSTM.
%% \item We do by recovering coherent spans of arguments (ones which start with \label{B-A_i}), and producing an extraction per each combination in the Cartesian product of arguments.
%% \item Multiple proposition per target predicate therefore occur when there is more than one instantiation of an argument type.
%% \item For example: @
%% \end{itemize}

%\todo{check and complete details if neded, such as POS tool you used}
During inference, we first identify all verbs in the sentence using a POS tagger and generate an input instance for the LSTM transducer per each verb.
For a given instance, we regard the most probable LSTM label assignment for each word as the predicted encoding for the sentence, and decode Open IE extractions from these labels.
We do so by recovering coherent spans for the predicate and arguments, starting with a Beginning label (e.g., \oielabel{A1-B}) and continuing with corresponding Inside labels.
Then, if each argument index occurs only once we simply generate a single extraction tuple headed by the predicate and including all arguments. If some argument indexes appear more than once, this corresponds to the case where a single predicate generates multiple extractions, as discussed at the encoding phase (end of Subsection \ref{sec:encoding}). Accordingly, we generate multiple extraction tuples, each one headed by the target predicate and containing one possible combination of arguments, choosing one argument for each position index. Overall, decoding reverses the encoding process, this time from BIO labels to extractions, as illustrated in Table \ref{tab:oie_examples}.


\paragraph{Assigning extraction confidence}
As discussed in Section \ref{sec:background}, it is beneficial for Open IE systems to associate a confidence value with each predicted extraction.
In our case, the LSTM assigns a probability to each label prediction, for each word.
We experimented with several heuristics to combine these predictions to an extraction level confidence metric.
The best performance on the development set was achieved by multiplying the word-level probabilities of each
of the elements participating in the extraction.\footnote{Other variants we tried were taking the maximum or
minimum observed probability, but that performed worse.}

We note that this metric clearly prefers shorter extractions.
While this is beneficial for the task of Open IE, which aims to reduce proposition span, an interesting
avenue for future research is developing a more principled way of assigning confidence scores to extractions.


\subsection{Implementation details}
We implemented the bi-LSTM transducer model using the \keras\ framework \cite{chollet2015} with a \tensorflow\ backend \cite{tensorflow2015-whitepaper}.
We use 3 layers of stacked bi-LSTMs, where each LSTM cell is composed of 128 hidden units
with a linear rectifier (ReLU) \cite{nair2010rectified} activation function.
Training was done on 100 epochs, in mini batches of 50 samples, with 10\% word level
dropout.
Word embeddings were initialized using the pre-trained embeddings computed in \newcite{pennington2014glove}, with loss propagating back to the word representation during training, hopefully
allowing them to assimilate a task-specific representation.

Finally, we use the average perceptron part-of-speech tagger (as implemented in spaCy\footnote{\url{https://spacy.io}})
to predict the part of speech for both feature computation and predicate identification (identifying verbs).
