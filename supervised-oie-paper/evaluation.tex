%% \begin{itemize}
%% \item We implemented the bi-LSTM transducer model using the \keras\ framework \cite{chollet2015} with a \tensorflow\ backend \cite{tensorflow2015-whitepaper}.
%%   We use 3 layers of stacked bi-LSTMs, where LSTM cell is composed of @ hidden units.
%%   for the LSTM we use

%%   \item Dimensions of layers
%%   \item activation functions
%%   \item Average perceptron part of speech tagging (as implemented in spaCy\footnote{\url{https://spacy.io}}
%%   \item Word embedding intialization using the pre-trained embeddings computed in
%%     \newcite{pennington2014glove}.
%% \end{itemize}

When evaluating Open IE it is desired to allow some room for flexibility when comparing against gold annotated extractions, for several reasons.
First, as we mentioned previously, current prominent Open IE systems were developed without a large reference
corpus. Following, this created some deviation in design and representation choices between the systems.
For example, ClausIE chooses to predict only extractions with two arguments (versus n-ary extractions in the other systems), and PropS includes prepositions as part of the predicate while Open IE4 posits them in the respective
argument slot.

Second, like other tasks that require subtle semantic decisions, such as summarization or machine translation,
there are often several possible  extractions that might be regarded as correct, and we do not want to penalize systems for not
recovering the specific choice made in the gold corpus.
For example, for the sentence \sent{The sheriff standing against the wall spoke in a very soft voice} the two following
extractions can be considered acceptable: \extraction{The Sheriff}{spoke}{in a soft voice},
\extraction{The sheriff standing against the wall}{spoke}{in a very soft voice}

To account for  this variance in predictions, we follow \newcite{hequestion} and \newcite{Stanovsky2016EMNLP} which judge an argument as correct if and only if it includes the \emph{syntactic head} of the gold argument.\footnote{We make use of the test suite published in \url{https://github.com/gabrielStanovsky/oie-benchmark}}
This would accept both variations of the previous example.
Note that this relaxed matching allows for fair comparison with rule-based systems which did not use a training
set to fit their prediction according to a strict matching function.

\subsection{Results}

\input{figures/auc_predicate.tex}
\input{figures/pr_all}
\input{figures/pr_predicate}
\input{figures/pr_args}
\input{figures/seen_vs_unseen}


\input{figures/pr_curves}

The Precision-Recall (PR) curve of our model (marked RnnOIE) is shown in Figure \ref{fig:evaluation},
using the soft matching function discussed above.
For comparison, we plot the performance of the top performing systems evaluated in \cite{Stanovsky2016EMNLP} on the same corpus. We note that these are very strong baseline systems, which are the result of over a decade of research.

RnnOIE outperforms the other systems in terms of precision on most of the recall range, and comes in second after (the much less precise) ClausIE in terms of maximum obtained recall (.63 versus .74).
Superior precision performance is especially attractive for
applications that employ Open IE over large amounts of data, in which a high degree of data redundancy is expected, such as in different news reports of the same event.

Following, the Area Under the PR-Curve (AUC) shows that overall our system
produces the best combination of aggregated precision and recall with a statistically significant margin over
the other systems. In scalar terms of the AUC, our improvement over state-of-the-art is similar in magnitude to previous
advancements in the field (4 points).


%\todo{it's worth mentioning the statistical test used}

%% \begin{itemize}
%% \item Figures:
%%   \begin{itemize}
%%   \item Optimizing precision vs. recall.
%%   \item In \& Out of domain?
%%   \item Number of extractions per system
%%   \item Check whether its common to report ``intrinsic'' numbers
%%   \end{itemize}
%% \end{itemize}

\input{figures/systems_stats}

\subsection{Analysis}
We describe several observations about the performance of the evaluated systems.

\paragraph{RnnOIE and Open IE4 produce shorter arguments.}
In Table \ref{tab:output_stats} we examine some properties of the output of different systems compared to the corresponding gold data.
This allows us to gain some insight into the performance of the systems.
For example, we can see that the best performing systems (RnnOIE and OpenIE4), tend to produce
more arguments, each of which tending to be shorter in terms of word count.
This observation fits the \emph{minimality} principle of Open IE, discussed in Section \ref{sec:background}.
Comparing to the same properties in the gold data set (first row), we can see that both systems
tend to overproduce and over-shorten their arguments.
% Compared to 2.45 arguments of 26.91 word length in average for the gold data set,
% RnnOIE and Open IE4 produce 3.07 and 3.19 arguments of word length 23.4 and 22.75 in average, respectively.

\paragraph{RnnOIE is able to generalize to unseen predicates.}
We analyze the extent to which our model overfits for the predicates seen during training, that is, whether it achieves good performance by memorizing specific patterns for these predicates.
To that end we have split the propositions in the gold and predicted test set into two corresponding partitions, namely ``seen'' and ``unseen'',
based on whether the predicate lemma appears in the training set or not.
The ``unseen'' part of the test set contains 145 unique predicate lemmas, constituting
24\% out of a total of 590 unique predicate lemmas in the entire test set.
Furthermore, it is composed of a total of 148 propositions (7\% out of 1993 in the entire test).
%% \todo{the above sentence isn't so clear, not sure what you meant to say. Do you refer to the fact that in 148 unseen propositions there were 145 different predicates? This isn't that surprising - events not seen in training are rare and are likely to be singletons in test. If that's the case maybe you can drop this sentence on the variability, not sure if it contributes to the main point here on unseen}

Following, we tested each of the predicted test partitions against its corresponding gold
counterpart.
The resulting PR curves (Figure \ref{fig:unseen}) depict overall good performance on the ``unseen'' partition,
We note, however, that the ``seen'' part is much larger, constituting roughly 93\% of the test data.
Overall, this indicates that the model indeed generalizes beyond the memorization of specific predicate templates, as
it managed to predict extractions for unseen predicates with good accuracy.

\begin{figure}[tb!]

  \centering
    \includegraphics[width=1\columnwidth]{figures/uniq_test_pr}

  \caption{Performance of our model on seen versus unseen test predicates.}
  \label{fig:unseen}

\end{figure}


%% \begin{itemize}
%% \item Performace on ``core'' arguments vs. ``adjuncts''
%% \item Standard error analysis: Sample 50 FN and 50 FP
%% \item Check what number of predicates are unseen in the test in each of these and do they affect performance
%% \end{itemize}
